# ðŸ¤– Git for Machine Learning Projects

## Overview

Learn specialized Git practices for machine learning projects, including what to version control, handling large files, data versioning strategies, model management, and experiment tracking.

---

## ðŸ“š Table of Contents

1. [What to Version Control](#what-to-version-control)
2. [Comprehensive .gitignore for ML](#comprehensive-gitignore-for-ml)
3. [Handling Large Files](#handling-large-files)
4. [Data Versioning Strategies](#data-versioning-strategies)
5. [Model Versioning](#model-versioning)
6. [Experiment Tracking](#experiment-tracking)
7. [Reproducibility](#reproducibility)
8. [ML Project Structure](#ml-project-structure)
9. [CI/CD for ML](#cicd-for-ml)
10. [Exercises](#exercises)

---

## âœ… What to Version Control

### **DO** Version Control These

```bash
# âœ… Source code
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ model.py
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â””â”€â”€ utils.py

# âœ… Configuration files
config/
â”œâ”€â”€ config.yaml
â”œâ”€â”€ hyperparameters.json
â””â”€â”€ data_config.py

# âœ… Environment specifications
requirements.txt
environment.yml
Dockerfile

# âœ… Small reference datasets (<10MB)
data/
â”œâ”€â”€ sample.csv          # âœ… Small sample
â””â”€â”€ data_description.txt

# âœ… Model architecture code (not weights!)
models/
â””â”€â”€ architecture.py     # âœ… Code
    # model.h5         # âŒ Weights

# âœ… Scripts and notebooks
scripts/
â”œâ”€â”€ download_data.py
â”œâ”€â”€ preprocess.py
â””â”€â”€ visualize.py

notebooks/
â””â”€â”€ exploration.ipynb   # âœ… Cleared output

# âœ… Documentation
README.md
CONTRIBUTING.md
docs/
â””â”€â”€ api_reference.md

# âœ… Tests
tests/
â”œâ”€â”€ test_model.py
â”œâ”€â”€ test_preprocessing.py
â””â”€â”€ test_utils.py

# âœ… CI/CD configuration
.github/
â””â”€â”€ workflows/
    â””â”€â”€ test.yml
```

### **DON'T** Version Control These

```bash
# âŒ Large datasets
data/
â”œâ”€â”€ train.csv          # 5GB
â”œâ”€â”€ test.csv           # 2GB
â””â”€â”€ images/            # 50GB
    # Use DVC or Git LFS instead!

# âŒ Trained model weights
models/
â”œâ”€â”€ model_v1.h5        # 500MB
â”œâ”€â”€ model_v2.pkl       # 200MB
â””â”€â”€ checkpoint.pth     # 1GB
    # Use DVC, MLflow, or cloud storage!

# âŒ Experiment outputs
runs/
â””â”€â”€ experiment_001/
    â”œâ”€â”€ logs/
    â”œâ”€â”€ checkpoints/
    â””â”€â”€ outputs/

# âŒ Virtual environments
venv/
env/
.venv/
ENV/

# âŒ Compiled files
__pycache__/
*.pyc
*.pyo
*.so
*.dll

# âŒ IDE files
.vscode/
.idea/
*.swp
.DS_Store

# âŒ Secrets and credentials
.env
credentials.json
*.key
*.pem
config/secrets.yaml

# âŒ Logs
logs/
*.log
tensorboard/
wandb/

# âŒ Temporary files
*.tmp
*.temp
.cache/
```

---

## ðŸš« Comprehensive .gitignore for ML

### Complete ML .gitignore

```.gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environments
venv/
ENV/
env/
.venv
env.bak/
venv.bak/

# Jupyter Notebook
.ipynb_checkpoints
*/.ipynb_checkpoints/*
*.ipynb_checkpoints/

# IPython
profile_default/
ipython_config.py

# PyCharm
.idea/
*.iml
*.iws

# VS Code
.vscode/
*.code-workspace

# Sublime Text
*.sublime-project
*.sublime-workspace

# Vim
*.swp
*.swo
*~

# macOS
.DS_Store
.AppleDouble
.LSOverride

# Windows
Thumbs.db
Thumbs.db:encryptable
ehthumbs.db
ehthumbs_vista.db
*.stackdump
[Dd]esktop.ini

# Data files
data/
*.csv
*.tsv
*.dat
*.json
*.jsonl
*.xml
*.sql
*.db
*.sqlite
*.sqlite3
*.h5
*.hdf5
*.parquet

# Keep sample data
!data/sample/
!data/sample/*.csv

# Model files
models/
*.h5
*.hdf5
*.pkl
*.pickle
*.pt
*.pth
*.onnx
*.pb
*.tflite
*.joblib
checkpoint*
*.ckpt

# Keep model architecture code
!models/*.py

# Logs and outputs
logs/
*.log
runs/
outputs/
results/
experiments/

# Experiment tracking
wandb/
mlruns/
.neptune/
.clearml/
aim/

# TensorBoard
events.out.tfevents.*
tensorboard/

# PyTorch
*.pt
*.pth
lightning_logs/

# TensorFlow
*.pb
*.meta
*.index
*.data-*
saved_model/

# Environment variables
.env
.env.local
.env.*.local

# Secrets
*.key
*.pem
*_credentials.json
secrets/
config/secrets.*

# Large files (use Git LFS or DVC)
*.zip
*.tar.gz
*.rar
*.7z
*.bin

# Cache
.cache/
*.cache
.pytest_cache/
.mypy_cache/
.ruff_cache/

# Coverage reports
htmlcov/
.coverage
.coverage.*
coverage.xml
*.cover

# Profiling
*.prof
*.lprof

# Documentation builds
docs/_build/
docs/_static/
docs/_templates/
site/

# Compiled extensions
*.pyd

# Distribution
*.whl
```

---

## ðŸ“¦ Handling Large Files

### Problem with Large Files

```bash
# Git stores entire history
# Large files slow down:
# - Cloning
# - Fetching
# - Pushing
# - Local operations

# Example:
git add large_model.h5  # 500MB
git commit -m "Add model"
# Now everyone who clones gets 500MB
# Even if file is later removed!
```

### Solutions

**1. Don't commit large files**
```bash
# Add to .gitignore
echo "*.h5" >> .gitignore
echo "models/" >> .gitignore
git add .gitignore
git commit -m "Ignore large model files"
```

**2. Use Git LFS**
```bash
git lfs install
git lfs track "*.h5"
git add .gitattributes
git add large_model.h5
git commit -m "Add model with LFS"
```

**3. Use DVC**
```bash
dvc add data/train.csv
git add data/train.csv.dvc data/.gitignore
git commit -m "Track data with DVC"
```

**4. Cloud Storage**
```bash
# Store on S3, GCS, Azure Blob Storage
# Track metadata in Git
{
  "model": "model_v1",
  "location": "s3://bucket/models/model_v1.h5",
  "metrics": {"accuracy": 0.95}
}
```

### Removing Large Files from History

```bash
# Use BFG Repo-Cleaner (faster than git filter-branch)
# Download from: https://rtyley.github.io/bfg-repo-cleaner/

# Clone repository (mirror)
git clone --mirror https://github.com/user/repo.git

# Remove large files
java -jar bfg.jar --strip-blobs-bigger-than 100M repo.git

# Clean up
cd repo.git
git reflog expire --expire=now --all
git gc --prune=now --aggressive

# Push cleaned history (FORCE PUSH!)
git push --force
```

---

## ðŸ’¾ Data Versioning Strategies

### Strategy 1: Data Snapshots with DVC

```bash
# Install DVC
pip install dvc

# Initialize
dvc init

# Track dataset
dvc add data/train.csv
# Creates data/train.csv.dvc
# Adds data/train.csv to .gitignore

# Commit DVC file
git add data/train.csv.dvc data/.gitignore
git commit -m "Track training data (v1)"

# Configure remote storage
dvc remote add -d myremote s3://mybucket/dvcstore

# Push data
dvc push

# Others can pull
git clone https://github.com/user/repo.git
dvc pull
```

### Strategy 2: Data Registries

```bash
# Separate data repository
data-registry/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ dataset_v1.csv.dvc
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ dataset_v1_processed.csv.dvc
â””â”€â”€ README.md

# Main project references data registry
git submodule add https://github.com/user/data-registry.git data
```

### Strategy 3: Data Manifests

```yaml
# data_manifest.yaml
datasets:
  training:
    version: "v1.0"
    location: "s3://bucket/data/train_v1.csv"
    hash: "md5:abc123..."
    size: "5GB"
    samples: 1000000
    
  validation:
    version: "v1.0"
    location: "s3://bucket/data/val_v1.csv"
    hash: "md5:def456..."
    size: "1GB"
    samples: 200000
```

### Strategy 4: DVC Pipelines

```yaml
# dvc.yaml
stages:
  download:
    cmd: python src/download.py
    deps:
      - src/download.py
    outs:
      - data/raw/

  preprocess:
    cmd: python src/preprocess.py
    deps:
      - src/preprocess.py
      - data/raw/
    outs:
      - data/processed/

  train:
    cmd: python src/train.py
    deps:
      - src/train.py
      - data/processed/
    outs:
      - models/model.pkl
    metrics:
      - metrics.json:
          cache: false
```

---

## ðŸŽ¯ Model Versioning

### Versioning Model Architecture

```python
# models/model_v1.py - Version control this!
class ModelV1:
    """Linear model baseline."""
    def __init__(self, input_dim):
        self.model = Sequential([
            Dense(64, activation='relu', input_dim=input_dim),
            Dense(1, activation='sigmoid')
        ])

# models/model_v2.py
class ModelV2:
    """Added dropout for regularization."""
    def __init__(self, input_dim):
        self.model = Sequential([
            Dense(64, activation='relu', input_dim=input_dim),
            Dropout(0.5),
            Dense(1, activation='sigmoid')
        ])
```

### Versioning Model Weights

```bash
# Method 1: DVC
dvc add models/model_v1.h5
git add models/model_v1.h5.dvc
git commit -m "Model v1: baseline (accuracy: 0.85)"
git tag -a model-v1 -m "Baseline model"

# Method 2: MLflow
import mlflow

with mlflow.start_run():
    mlflow.log_param("model_type", "RandomForest")
    mlflow.log_metric("accuracy", 0.85)
    mlflow.sklearn.log_model(model, "model")

# Method 3: Model registry
{
  "model_id": "model_v1",
  "version": "1.0.0",
  "created": "2024-01-15",
  "location": "s3://bucket/models/model_v1.h5",
  "metrics": {
    "accuracy": 0.85,
    "f1_score": 0.82
  },
  "hyperparameters": {
    "learning_rate": 0.001,
    "epochs": 100
  }
}
```

### Tagging Model Versions

```bash
# Tag significant model versions
git tag -a model-v1.0 -m "Baseline model: accuracy 0.85"
git tag -a model-v1.1 -m "Added regularization: accuracy 0.87"
git tag -a model-v2.0 -m "New architecture: accuracy 0.92"

# Push tags
git push --tags

# Checkout specific model version
git checkout model-v1.0
```

---

## ðŸ§ª Experiment Tracking

### Organizing Experiments

```bash
project/
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ exp_001_baseline/
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ results.json
â”‚   â”œâ”€â”€ exp_002_dropout/
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ results.json
â”‚   â””â”€â”€ README.md  # Experiment log
```

### Experiment Log

```markdown
# Experiment Log

## Experiment 001: Baseline
- **Date**: 2024-01-15
- **Goal**: Establish baseline performance
- **Model**: Linear regression
- **Results**: MSE=0.25, RÂ²=0.75
- **Commit**: abc123d

## Experiment 002: Ridge Regression
- **Date**: 2024-01-16
- **Goal**: Reduce overfitting
- **Model**: Ridge (alpha=1.0)
- **Results**: MSE=0.22, RÂ²=0.78
- **Improvement**: +4% RÂ²
- **Commit**: def456e
```

### Experiment Tracking Tools

```python
# Weights & Biases (wandb)
import wandb

wandb.init(project="ml-project", name="exp_001")
wandb.config.update({"learning_rate": 0.001, "epochs": 100})

for epoch in range(100):
    # Training...
    wandb.log({"loss": loss, "accuracy": acc})

# MLflow
import mlflow

mlflow.set_experiment("ml-project")
with mlflow.start_run(run_name="exp_001"):
    mlflow.log_params({"lr": 0.001, "epochs": 100})
    # Training...
    mlflow.log_metrics({"loss": loss, "accuracy": acc})

# TensorBoard
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/exp_001')
writer.add_scalar('Loss/train', loss, epoch)
writer.add_scalar('Accuracy/train', acc, epoch)
writer.close()
```

---

## ðŸ”¬ Reproducibility

### Requirements Files

```bash
# Exact versions
numpy==1.21.0
pandas==1.3.2
scikit-learn==0.24.2

# Or use poetry/conda for lock files
poetry.lock
conda-lock.yml
```

### Random Seeds

```python
# seed_config.py
import random
import numpy as np
import torch

def set_seed(seed=42):
    """Set random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

### Configuration Management

```yaml
# config.yaml
data:
  train_path: data/train.csv
  val_path: data/val.csv
  test_path: data/test.csv
  
model:
  architecture: resnet50
  pretrained: true
  num_classes: 10

training:
  batch_size: 32
  learning_rate: 0.001
  epochs: 100
  optimizer: adam
  
reproducibility:
  seed: 42
  deterministic: true
```

### Dockerfiles

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY src/ ./src/
COPY models/ ./models/
COPY config/ ./config/

# Set environment
ENV PYTHONPATH=/app

# Run
CMD ["python", "src/train.py"]
```

---

## ðŸ“ ML Project Structure

### Recommended Structure

```bash
ml-project/
â”œâ”€â”€ .git/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dvcignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ data/                    # Data (ignored by Git, tracked by DVC)
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ .gitkeep            # Track empty directory
â”‚
â”œâ”€â”€ models/                  # Model weights (ignored)
â”‚   â”œâ”€â”€ architecture.py      # âœ… Version controlled
â”‚   â””â”€â”€ model_v1.h5          # âŒ Use DVC
â”‚
â”œâ”€â”€ notebooks/               # Experiments
â”‚   â”œâ”€â”€ 01_exploration.ipynb
â”‚   â”œâ”€â”€ 02_baseline.ipynb
â”‚   â””â”€â”€ 03_improvements.ipynb
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â””â”€â”€ preprocess.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ tests/                   # Unit tests
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_model.py
â”‚   â””â”€â”€ test_utils.py
â”‚
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ hyperparameters.json
â”‚
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ download_data.sh
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ API.md
â”‚   â””â”€â”€ TRAINING.md
â”‚
â””â”€â”€ .github/                 # CI/CD
    â””â”€â”€ workflows/
        â””â”€â”€ test.yml
```

---

## ðŸš€ CI/CD for ML

### GitHub Actions for ML

```yaml
# .github/workflows/ml-pipeline.yml
name: ML Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run tests
        run: pytest --cov=src tests/
      
      - name: Lint
        run: |
          pip install black flake8
          black --check src/
          flake8 src/
  
  train:
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3
      
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Setup DVC
        uses: iterative/setup-dvc@v1
      
      - name: Pull data
        run: dvc pull
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      
      - name: Train model
        run: python scripts/train.py
      
      - name: Upload model
        uses: actions/upload-artifact@v3
        with:
          name: trained-model
          path: models/model.pkl
```

---

## ðŸ’ª Exercises

### Exercise 1: Setup ML Project

Create properly structured ML project with Git.

**Solution:**

```bash
# Create structure
mkdir ml-project && cd ml-project
git init

# Create .gitignore
curl -o .gitignore https://raw.githubusercontent.com/github/gitignore/main/Python.gitignore

# Add ML-specific ignores
cat >> .gitignore << 'EOF'
data/
models/*.h5
models/*.pkl
logs/
wandb/
EOF

# Create structure
mkdir -p {src,tests,config,notebooks,scripts}
mkdir -p src/{data,models,evaluation}

# Create placeholder files
touch src/__init__.py
touch src/data/__init__.py
touch src/models/__init__.py

# Initial commit
git add .
git commit -m "Initial project structure"
```

### Exercise 2: DVC Setup

Setup DVC for data versioning.

**Solution:**

```bash
# Install DVC
pip install dvc

# Initialize
git init
dvc init
git add .dvc .dvcignore
git commit -m "Initialize DVC"

# Track data
dvc add data/train.csv

# Commit DVC file
git add data/train.csv.dvc data/.gitignore
git commit -m "Track training data v1"

# Setup remote (S3 example)
dvc remote add -d myremote s3://mybucket/dvcstore
git add .dvc/config
git commit -m "Configure DVC remote"

# Push data
dvc push
```

### Exercise 3: Model Versioning

Implement complete model versioning workflow.

**Solution:**

```bash
# Train and save model
python train.py  # Saves models/model_v1.h5

# Track with DVC
dvc add models/model_v1.h5
git add models/model_v1.h5.dvc models/.gitignore

# Create metadata
cat > models/model_v1.json << 'EOF'
{
  "version": "1.0",
  "accuracy": 0.85,
  "created": "2024-01-15",
  "hyperparameters": {
    "learning_rate": 0.001,
    "epochs": 100
  }
}
EOF

# Commit
git add models/model_v1.json
git commit -m "Model v1.0: baseline (accuracy: 0.85)"

# Tag
git tag -a model-v1.0 -m "Baseline model"

# Push
git push --tags
dvc push
```

---

## ðŸŽ¯ Key Takeaways

âœ… **Version Control**: Code, configs, small data - NOT large files or secrets  
âœ… **.gitignore**: Comprehensive ML-specific ignore rules  
âœ… **Large Files**: Use Git LFS or DVC, not plain Git  
âœ… **Data Versioning**: DVC, data registries, or manifests  
âœ… **Model Versioning**: Separate architecture (code) from weights (DVC)  
âœ… **Experiments**: Track with MLflow, Wandb, or experiment logs  
âœ… **Reproducibility**: Pin dependencies, set seeds, use configs  
âœ… **Structure**: Organized project layout for collaboration  
âœ… **CI/CD**: Automate testing and training pipelines

---

## ðŸ”— Navigation

- **Previous**: [03-Remote-Repositories.md](./03-Remote-Repositories.md)
- **Next**: [05-Git-LFS.md](./05-Git-LFS.md)
- **Up**: [README.md](./README.md)
- **Chapter Home**: [Chapter 3 - Development Environment](../README.md)

---

**Happy ML Version Control! ðŸ¤–ðŸš€**
