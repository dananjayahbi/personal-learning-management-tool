# 1.3 Probability Theory - README

## ðŸ“š Section Overview

Probability theory is essential for understanding uncertainty in machine learning. From Naive Bayes to probabilistic graphical models, probability underpins modern ML.

---

## ðŸ“‘ Topics Covered

1. âœ… **[Basic Probability Concepts](./01-Basic-Probability-Concepts.md)** - COMPLETE
   - Sample spaces and events
   - Probability axioms  
   - Conditional probability
   - Bayes' theorem
   - Independence
   - Spam classification example

2. **Random Variables** - Coming Soon
   - Discrete vs continuous
   - PMF and PDF
   - CDF and quantiles
   - Expected value and variance
   - Moment generating functions

3. **Common Distributions** - Coming Soon
   - Bernoulli and Binomial
   - Normal (Gaussian)
   - Exponential and Poisson
   - Beta and Dirichlet
   - Applications in ML

4. **Joint Distributions** - Coming Soon
   - Joint probability
   - Marginal and conditional distributions
   - Covariance and correlation
   - Multivariate normal

5. **Applications in ML** - Coming Soon
   - Naive Bayes classifier
   - Maximum likelihood estimation
   - Bayesian inference
   - Probabilistic models

---

## ðŸŽ¯ Why Probability Matters in ML

- **Uncertainty Quantification**: Models make probabilistic predictions
- **Bayesian Methods**: Prior + Likelihood â†’ Posterior
- **Generative Models**: Learn probability distributions
- **Decision Making**: Optimal decisions under uncertainty

---

## ðŸ’¡ Bayes' Theorem

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

The most important formula in machine learning!

---

**Next**: Continue with random variables or review basic concepts.
